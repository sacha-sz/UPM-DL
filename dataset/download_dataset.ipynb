{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sacha\\anaconda3\\envs\\SY32\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apod(url, adir, sess):\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "    }\n",
    "    if not os.path.exists(adir):\n",
    "        os.makedirs(adir, exist_ok=False)\n",
    "\n",
    "    parenturl = os.path.split(url)[0]\n",
    "    spaceregex = re.compile(r\"\\s{2,}\")\n",
    "\n",
    "    try:\n",
    "        apod = sess.get(url, timeout=30, headers=headers, verify=True)\n",
    "        apod.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting page {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    apodsoup = BeautifulSoup(apod.text, features=\"lxml\")\n",
    "    imgelem = apodsoup.find_all(\"a\", href=re.compile(\"^image\"))\n",
    "\n",
    "    if imgelem == []:\n",
    "        print(f\"No image link found for {url}\")\n",
    "        return None\n",
    "    else:\n",
    "        imgurl = parenturl + \"/\" + imgelem[0].get(\"href\")\n",
    "        imgfilename = os.path.basename(imgurl)\n",
    "\n",
    "        # Vérifie si le fichier existe déjà\n",
    "        if not os.path.exists(os.path.join(adir, imgfilename)):\n",
    "            try:\n",
    "                imageresp = sess.get(\n",
    "                    imgurl,\n",
    "                    headers=headers,\n",
    "                    timeout=30,\n",
    "                    stream=True,\n",
    "                    verify=True,\n",
    "                )\n",
    "                imageresp.raise_for_status()\n",
    "                \n",
    "                # Redimensionnement de l'image\n",
    "                img = Image.open(BytesIO(imageresp.content))\n",
    "                img.thumbnail((256, 256))  # Redimensionner l'image à 256x256 max\n",
    "\n",
    "                # Enregistre l'image redimensionnée\n",
    "                with open(os.path.join(adir, imgfilename), \"wb\") as fd:\n",
    "                    img.save(fd)\n",
    "                imageresp.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading image {imgurl}: {e}\")\n",
    "\n",
    "        imgdate = imgelem[0].find_previous(\"p\").getText(strip=True)\n",
    "        img_obj = datetime.strptime(imgdate, \"%Y %B %d\")\n",
    "        imgdate = img_obj.strftime(\"%d-%m-%Y\")\n",
    "        imgtitle = imgelem[0].find_next(\"b\").getText(strip=True)\n",
    "        imgtext = imgelem[0].find_next(\"p\").getText()\n",
    "        imgtext = re.sub(\"\\n\", \" \", imgtext).strip()\n",
    "        imgtext = re.sub(spaceregex, \" \", imgtext)\n",
    "        \n",
    "        # Ajout des informations dans le fichier CSV avec l'URL\n",
    "        csv_file = \"credits.csv\"\n",
    "        file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "        with open(csv_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not file_exists:\n",
    "                writer.writerow(['Date', 'Title', 'Filename', 'Source_URL', 'Description'])\n",
    "    \n",
    "            writer.writerow([imgdate, imgtitle, imgfilename, url, imgtext])            \n",
    "\n",
    "        return imgdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_url(date):\n",
    "    return f\"https://apod.nasa.gov/apod/ap{date.strftime('%y%m%d')}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_apod_images(start_date, end_date, adir=\"apod-images\", max_workers=5):\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    current_date = start_date\n",
    "    urls = []\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        urls.append(date_to_url(current_date))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f\"Début du téléchargement des images APOD du {start_date} au {end_date} dans {adir} avec {max_workers} threads\")\n",
    "    with requests.Session() as sess:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(get_apod, url, adir, sess): url for url in urls}\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading APOD images\", miniters=100):\n",
    "                url = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f\"Error downloading {url}: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du téléchargement des images APOD du 2022-01-01 00:00:00 au 2022-01-05 00:00:00 dans download_test avec 10 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading APOD images:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sacha\\AppData\\Local\\Temp\\ipykernel_18748\\563877456.py:43: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  credits_elem = apodsoup.find(\"b\", text=\"Image Credit &\").find_next(\"a\")\n",
      "Downloading APOD images: 100%|██████████| 5/5 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading https://apod.nasa.gov/apod/ap220105.html: 'NoneType' object has no attribute 'find_next'\n",
      "Error downloading https://apod.nasa.gov/apod/ap220101.html: 'NoneType' object has no attribute 'find_next'\n",
      "Error downloading https://apod.nasa.gov/apod/ap220103.html: 'NoneType' object has no attribute 'find_next'\n",
      "Error downloading https://apod.nasa.gov/apod/ap220102.html: 'NoneType' object has no attribute 'find_next'\n",
      "Error downloading https://apod.nasa.gov/apod/ap220104.html: 'NoneType' object has no attribute 'find_next'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Paramètres par défaut\n",
    "SAVEDIR = \"download_test\"\n",
    "start_date = \"2022-01-01\"  # Date de début au format AAAA-MM-JJ\n",
    "end_date = \"2022-01-05\"    # Date de fin au format AAAA-MM-JJ\n",
    "\n",
    "def get_apod(url, adir, sess):\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "    }\n",
    "    \n",
    "    # Crée le répertoire si nécessaire\n",
    "    if not os.path.exists(adir):\n",
    "        os.makedirs(adir, exist_ok=False)\n",
    "    \n",
    "    parenturl = os.path.split(url)[0]\n",
    "    \n",
    "    try:\n",
    "        apod = sess.get(url, timeout=30, headers=headers, verify=True)\n",
    "        apod.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting page {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    apodsoup = BeautifulSoup(apod.text, features=\"lxml\")\n",
    "    imgelem = apodsoup.find_all(\"a\", href=re.compile(\"^image\"))\n",
    "\n",
    "    if not imgelem:\n",
    "        print(f\"No image link found for {url}\")\n",
    "        return None\n",
    "    else:\n",
    "        # Récupération des informations\n",
    "        imgurl = parenturl + \"/\" + imgelem[0].get(\"href\")\n",
    "        imgfilename = os.path.basename(imgurl)\n",
    "        imgdate = imgelem[0].find_previous(\"p\").getText(strip=True)\n",
    "        \n",
    "        # Extraction des crédits, auteur, titre depuis le HTML\n",
    "        title = apodsoup.find(\"title\").get_text(strip=True)\n",
    "        credits_elem = apodsoup.find(\"b\", text=\"Image Credit &\").find_next(\"a\")\n",
    "        credits = credits_elem.get_text(strip=True) if credits_elem else \"Unknown\"\n",
    "        \n",
    "        auteur_elem = credits_elem.find_next(\"a\") if credits_elem else None\n",
    "        auteur = auteur_elem.get_text(strip=True) if auteur_elem else \"Unknown\"\n",
    "        \n",
    "        # Ajout des informations dans le fichier CSV avec l'URL\n",
    "        csv_file = os.path.join(adir, \"credits.csv\")\n",
    "        file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not file_exists:\n",
    "                writer.writerow([\"nom_image\", \"credits\", \"auteur\", \"title\", \"date\", \"url_image\"])\n",
    "            \n",
    "            writer.writerow([imgfilename, credits, auteur, title, imgdate, imgurl])\n",
    "\n",
    "        return imgdate\n",
    "\n",
    "\n",
    "# Exécution\n",
    "download_apod_images(start_date, end_date, adir=SAVEDIR, max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(SAVEDIR, \"train_black_and_white\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVEDIR, \"train_color\"), exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(SAVEDIR), desc=\"Converting images to black and white and color\"):\n",
    "    file_path = os.path.join(SAVEDIR, filename)\n",
    "    if os.path.isfile(file_path):  # Ensure it's a file\n",
    "        img = Image.open(file_path).convert(\"L\")\n",
    "        img.save(os.path.join(SAVEDIR, \"train_black_and_white\", filename))\n",
    "        img = Image.open(file_path).convert(\"RGB\")\n",
    "        img.save(os.path.join(SAVEDIR, \"train_color\", filename))\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything in PNG\n",
    "for root, dirs, files in os.walk(\"dataset\"):\n",
    "    for file in tqdm(files, desc=\"Converting images to png\"):\n",
    "        extension = os.path.splitext(file)[1]\n",
    "        if extension in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\", \".bmp\", \".BMP\"]:\n",
    "            img = Image.open(os.path.join(root, file))\n",
    "            img.save(os.path.join(root, file.replace(extension, \".png\")))\n",
    "            os.remove(os.path.join(root, file))\n",
    "        elif extension in [\".gif\", \".GIF\"]:\n",
    "            # Keep only the first frame of the gif\n",
    "            img = Image.open(os.path.join(root, file))\n",
    "            img.seek(0)\n",
    "            img.save(os.path.join(root, file.replace(extension, \".png\")))\n",
    "            img.close()\n",
    "            os.remove(os.path.join(root, file)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_apod(url, adir, sess):\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "    }\n",
    "    \n",
    "    # Crée le répertoire si nécessaire\n",
    "    if not os.path.exists(adir):\n",
    "        os.makedirs(adir, exist_ok=False)\n",
    "    \n",
    "    parenturl = os.path.split(url)[0]\n",
    "    \n",
    "    try:\n",
    "        apod = sess.get(url, timeout=30, headers=headers, verify=True)\n",
    "        apod.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting page {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    apodsoup = BeautifulSoup(apod.text, features=\"lxml\")\n",
    "    imgelem = apodsoup.find_all(\"a\", href=re.compile(\"^image\"))\n",
    "\n",
    "    if not imgelem:\n",
    "        print(f\"No image link found for {url}\")\n",
    "        return None\n",
    "    else:\n",
    "        # Récupération des informations\n",
    "        imgurl = parenturl + \"/\" + imgelem[0].get(\"href\")\n",
    "        imgfilename = os.path.basename(imgurl)\n",
    "        imgdate = imgelem[0].find_previous(\"p\").getText(strip=True)\n",
    "        \n",
    "        # Extraction des crédits, auteur, titre depuis le HTML\n",
    "        title = apodsoup.find(\"title\").get_text(strip=True)\n",
    "        credits_elem = apodsoup.find(\"b\", text=\"Image Credit &\").find_next(\"a\")\n",
    "        credits = credits_elem.get_text(strip=True) if credits_elem else \"Unknown\"\n",
    "        \n",
    "        auteur_elem = credits_elem.find_next(\"a\") if credits_elem else None\n",
    "        auteur = auteur_elem.get_text(strip=True) if auteur_elem else \"Unknown\"\n",
    "        \n",
    "        # Ajout des informations dans le fichier CSV avec l'URL\n",
    "        csv_file = os.path.join(adir, \"credits.csv\")\n",
    "        file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not file_exists:\n",
    "                writer.writerow([\"nom_image\", \"credits\", \"auteur\", \"title\", \"date\", \"url_image\"])\n",
    "            \n",
    "            writer.writerow([imgfilename, credits, auteur, title, imgdate, imgurl])\n",
    "\n",
    "        return imgdate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SY32",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
